{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "train = pd.read_csv(\"C:\\\\Users\\\\joshn\\\\Documents\\\\Coding\\\\Spaceship Titanic\\\\Stacking\\\\Process\\\\CombinedTrain.csv\")\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\joshn\\\\Documents\\\\Coding\\\\Spaceship Titanic\\\\Stacking\\\\Process\\\\CombinedTest.csv\")\n",
    "\n",
    "# Separate ID before training\n",
    "train_passenger_ids = train['PassengerId']\n",
    "test_passenger_ids = test['PassengerId']\n",
    "\n",
    "# Prepare data\n",
    "X_train = train.drop(['Transported', 'PassengerId'], axis=1)\n",
    "y_train = train['Transported'].astype('int')  # Convert True/False to 1/0\n",
    "X_test = test.drop('PassengerId', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Best Parameters CV:  {'subsample': 0.7, 'reg_lambda': 1.1, 'reg_alpha': 0.1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.2, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=1)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of gradient boosted trees. Equivalent to the number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "    'max_depth': [3, 6, 9],  # Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit\n",
    "    'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'gamma': [0, 0.1, 0.2],  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    'subsample': [0.7, 0.8, 0.9],  # Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],  # Subsample ratio of columns when constructing each tree\n",
    "    'reg_alpha': [0, 0.01, 0.1],  # L1 regularization term on weights. Increasing this value will make model more conservative\n",
    "    'reg_lambda': [1, 1.1, 1.2],  # L2 regularization term on weights. Increasing this value will make model more conservative\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "random_search_cv = RandomizedSearchCV(xgb_model, param_grid, n_iter=200, cv=5, scoring='accuracy', verbose=1, random_state=1)\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params_cv = random_search_cv.best_params_\n",
    "print(\"Best Parameters CV: \", best_params_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XGBoost model with best parameters\n",
    "model = xgb.XGBClassifier(**best_params_cv, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Prepare submission\n",
    "test_passenger_ids = test['PassengerId']\n",
    "test_predictions = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Transported': y_test_pred\n",
    "})\n",
    "\n",
    "# Convert predictions to boolean\n",
    "test_predictions['Transported'] = test_predictions['Transported'].astype(bool)\n",
    "\n",
    "# Export to CSV\n",
    "test_predictions.to_csv(\"C:\\\\Users\\\\joshn\\\\Documents\\\\Coding\\\\Spaceship Titanic\\\\Stacking\\\\Test\\\\Submission - XGBoost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "train = pd.read_csv(\"C:\\\\Users\\\\joshn\\\\Documents\\\\Coding\\\\Spaceship Titanic\\\\Stacking\\\\Process\\\\CombinedTrain.csv\")\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\joshn\\\\Documents\\\\Coding\\\\Spaceship Titanic\\\\Stacking\\\\Process\\\\CombinedTest.csv\")\n",
    "\n",
    "# Separate ID before training\n",
    "train_passenger_ids = train['PassengerId']\n",
    "test_passenger_ids = test['PassengerId']\n",
    "\n",
    "# Prepare data\n",
    "X_train = train.drop(['Transported', 'PassengerId'], axis=1)\n",
    "y_train = train['Transported'].astype('int')  # Convert True/False to 1/0\n",
    "X_test = test.drop('PassengerId', axis=1)\n",
    "\n",
    "best_params_cv = {'subsample': 0.7, 'reg_lambda': 1.1, 'reg_alpha': 0.1, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.2, 'colsample_bytree': 0.8}\n",
    "\n",
    "# Create the XGBoost model with best parameters\n",
    "model = xgb.XGBClassifier(**best_params_cv, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Prepare submission\n",
    "test_passenger_ids = test['PassengerId']\n",
    "test_predictions = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Transported': y_test_pred\n",
    "})\n",
    "test_predictions['Transported'] = test_predictions['Transported'].astype(bool)\n",
    "\n",
    "# Export to CSV\n",
    "test_predictions.to_csv(\"C:\\\\Users\\\\joshn\\\\Documents\\\\Coding\\\\Spaceship Titanic\\\\Stacking\\\\Test\\\\Submission - XGBoost.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
